---
layout: home
order: 1
permalink: /workshop24
title: Workshop 2024
desc_title: Machine Learning and Compression Workshop @ NeurIPS 2024
social: true
---



### Call for Papers


The workshop solicits original research in the intersection of machine learning and data/model compression.

Machine learning and compression have been described as "two sides of the same coin", and the exponential amount of data being generated in diverse domains underscores the need for improved compression as well as efficient AI systems. Leveraging deep generative models, recent machine learning-based methods have set new benchmarks for compressing images, videos, and audio. Despite these advances, many open problems remain, such as computational efficiency, performance guarantees, and channel simulation. Parallel advances in large-scale foundation models further spurred research in efficient AI techniques such as model compression and distillation. This workshop aims to unite researchers from machine learning, data/model compression, and information theory. It will focus on enhancing compression techniques, accelerating large model training and inference, exploring theoretical limits, and integrating information-theoretic principles to improve learning and generalization. By bridging disciplines, we seek to catalyze the next generation of scalable, efficient information-processing systems.


Topics of interest include, but are not limited to,

* Improvements in learning-based techniques for compressing data, model weights, other (e.g., implicit) representations of signals, and emerging data modalities.
* Accelerating training and inference for large foundation models, potentially in distributed settings.
* Theoretical understanding of neural compression methods, including but not limited to fundamental information-theoretic limits, perceptual/realism metrics, distributed compression and compression without quantization.
* Understanding/improving learning and generalization via compression and information-theoretic principles.
* Information-theoretic aspects of unsupervised learning and representation learning.


<!--
### Call for Reviewers
Please fill out this [Google form](https://docs.google.com/forms/d/e/1FAIpQLSd3L9_o7vAZUSWjWMxi18jZHuIrBaafUBm6v1fTZQorK2o9Qw/viewform) if you are interested in reviewing for the workshop.

~~ðŸ† **2 free ICML 2023 workshop registrations will be given as "Best Reviewer Awards"** ðŸ†~~
-->

### Important Dates

* **Submission deadline**: Sept 05 11:59 PM AOE (anywhere on earth), 2024
* **Notification date**: Sept 30, 2024
* **Workshop date**: Dec 14 or Dec 15 (TBD), 2024

### Submission Instructions

<!--**Submission website: [OpenReview](https://openreview.net/group?id=ICML.cc/2023/Workshop/NCW)**-->
**Submission website: coming soon!**

We solicit short workshop paper submissions of up to 4 pages + unlimited references/appendices. Please format submissions in NeurIPS style. Submissions will be double blind: reviewers cannot see author names when conducting reviews, and authors cannot see reviewer names.

Some accepted papers will be accepted as contributed talks. All accepted posters are expected to be presented in-person at the poster session, and all papers published via Openreview after the workshop.

This workshop will not have formal proceedings, so we welcome the submission of work currently under review at other archival ML venues. We also welcome the submission of work recently published in information theory venues (e.g. Transactions on Information Theory, ISIT, ITW) that may be of interest to an ML audience. However, we will not consider work recently published in or accepted to other archival ML venues (e.g. ICML main conference).

<!-- Paper submissions should be made through OpenReview and further information will be available at [CFP]({{ site.baseurl }}{% link _pages/iclr21_call.md %}). Please send your inquiries by email to the organizers at [neural.compression.workshop@gmail.com](mailto:neural.compression.workshop@gmail.com). -->



### Speakers and Panelists
Coming soon!


### Organizers


<table style="width:75%">
  <tr>
    <td style="text-align:center"><img src="assets/img/yibo_yang.jpg" height="175"></td>
    <td style="text-align:center"><img src="assets/img/karen_ullrich.jpg" height="175"></td>
    <td style="text-align:center"><img src="assets/img/justus_will.jpg" height="175"></td>
    <td style="text-align:center"><img src="assets/img/ezgi_ozyilkan.jpg" height="175"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://yiboyang.com">Yibo Yang</a> <br> PhD Student, UC Irvine</td>
    <td style="text-align:center"><a href="https://karenullrich.info">Karen Ullrich</a> <br>Research Scientist, Meta AI</td>
    <td style="text-align:center"><a href="https://www.justuswill.com/">Justus Will</a> <br> PhD Student, UC Irvine</td>
    <td style="text-align:center"><a href="https://ezgimez.github.io/">Ezgi Ã–zyÄ±lkan</a> <br> PhD Student, New York University</td>
  </tr>
  <tr>
    <td style="text-align:center"><img src="assets/img/elza_erkip.jpg" height="175"></td>
    <td style="text-align:center"><img src="assets/img/stephan_mandt.jpg" height="175"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://wp.nyu.edu/elza_erkip/">Elza Erkip</a> <br> Professor, New York University</td>
    <td style="text-align:center"><a href="http://www.stephanmandt.com">Stephan Mandt</a> <br> Associate Professor, UC Irvine</td>
  </tr>
</table>


### Organizers affiliations
<td style="text-align:center"><img src="assets/img/logo_uci.jpg" height="100"></td>
<br>

<td style="text-align:center"><img src="assets/img/logo_meta.png" height="80"></td>
<br>

<td style="text-align:center"><img src="assets/img/logo_nyu.png" height="140"></td>

